{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "LoRA适合大模型的fine-tune\n",
    "\n",
    "小模型 冻结部分参数或全量微调\n",
    "\n",
    "低秩矩阵分解：\n",
    "* 假设预训练模型的权重矩阵  W  为一个固定的大矩阵。在 LoRA 中，模型不会直接更新  W ，而是将它分解为两个低秩矩阵：\n",
    "\n",
    "$W + \\Delta W = W + A \\cdot B$\n",
    "\n",
    "其中：\n",
    "* A  和  B  是小的可训练矩阵，秩  r  满足  $r \\ll d$ ， d  是矩阵的维度。\n",
    "* W  是预训练的固定权重，保持冻结状态。\n",
    "* $\\Delta W = A \\cdot B $ 是通过 LoRA 学习的低秩调整项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    #merge要不要用上预训练weight\n",
    "    #rank降到多少维\n",
    "    #lora_alpha比例\n",
    "    def __init__(self,in_features,out_features,merge,rank=16,lora_alpha=16,dropout=0.5):\n",
    "        super(LoRALinear,self).__init__()\n",
    "        self.in_features=in_features\n",
    "        self.out_features=out_features\n",
    "        self.merge=merge\n",
    "        self.rank=rank\n",
    "        self.lora_alpha=lora_alpha\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        self.linear=nn.Linear(in_features,out_features)\n",
    "        if rank>0:\n",
    "            self.lora_b=nn.Parameter(torch.zeros(out_features,rank))\n",
    "            self.lora_a=nn.Parameter(torch.zeros(rank,in_features))\n",
    "            self.scale=self.lora_alpha/self.rank\n",
    "            self.linear.weight.requires_grad=False\n",
    "        if dropout>0:\n",
    "            self.dropout=nn.Dropout(dropout)\n",
    "        else: \n",
    "            self.dropout=nn.Identity()\n",
    "            \n",
    "        self.initial_weights()\n",
    "        \n",
    "    def initial_weights(self):\n",
    "        nn.init.kaiming_normal_(self.lora_a,a=0.01)\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.rank>0 and self.merge:\n",
    "            output=F.linear(x,self.linear.weight+self.lora_b@self.lora_a*self.scale,self.linear.bias)\n",
    "            output=self.dropout(output)\n",
    "            return output\n",
    "        else:\n",
    "            return self.dropout(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, original_dim, rank):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.A = nn.Parameter(torch.randn(original_dim, rank))  # Low-rank matrix A\n",
    "        self.B = nn.Parameter(torch.randn(rank, original_dim))  # Low-rank matrix B\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.A)\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.A @ self.B\n",
    "    \n",
    "class LoRAAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, rank):\n",
    "        super(LoRAAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        # 原始权重 (冻结)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # LoRA 权重 (可训练)\n",
    "        self.lora_q = LoRA(d_model, rank)\n",
    "        self.lora_k = LoRA(d_model, rank)\n",
    "        self.lora_v = LoRA(d_model, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始权重计算\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        # 加入 LoRA 调整\n",
    "        q = q + self.lora_q(x)\n",
    "        k = k + self.lora_k(x)\n",
    "        v = v + self.lora_v(x)\n",
    "\n",
    "        # 自注意力计算 (省略实现细节)\n",
    "        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        output = attn_weights @ v\n",
    "        return output\n",
    "    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 加载预训练模型和标记器\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 配置 LoRA 参数\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                     # LoRA rank\n",
    "    lora_alpha=16,           # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 微调的层\n",
    "    lora_dropout=0.1,        # Dropout 概率\n",
    "    bias=\"none\",             # 是否添加偏置项\n",
    "    task_type=\"CAUSAL_LM\"    # 任务类型\n",
    ")\n",
    "\n",
    "# 将模型转为 LoRA 模式\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 打印可训练参数数量\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in lora_model.parameters() if p.requires_grad))\n",
    "\n",
    "# 数据集和训练（省略数据加载和训练过程）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
