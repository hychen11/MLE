{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是transform的手撕+code练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.nn.init as init\n",
    "init.kaiming_normal_(weights, mode='fan_in', nonlinearity='relu')\n",
    "init.xavier_normal_(input_,output_)\n",
    "\n",
    "nn.softmax就是网络里的一部分，而f.softmax只是一个函数\n",
    "\n",
    "所以区分一下torch.nn和torch.nn.functional\n",
    "\n",
    "torch.randn(input_,output_) #(0,1)\n",
    "torch.normal(mu,var,size=(input_,output_))\n",
    "f.relu()\n",
    "f.softmax()\n",
    "f.linear()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenhaoyang/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.randn(128,64,512)  #batch, time, dimension\n",
    "print(X.shape)\n",
    "d_model=512\n",
    "n_head=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention(nn.Module):  #需要继承nn.Module库\n",
    "    def __init__(self,d_model,n_head)->None:\n",
    "        super(multi_head_attention, self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_head=n_head\n",
    "        \n",
    "        self.w_q=nn.Linear(d_model,d_model)\n",
    "        self.w_k=nn.Linear(d_model,d_model)\n",
    "        self.w_v=nn.Linear(d_model,d_model)\n",
    "        self.fc_out=nn.Linear(d_model,d_model)\n",
    "        self.softmax=nn.Softmax(dim=-1)  #dim=-1只对最后一个维度进行操作\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch,time,dimension=x.shape\n",
    "        n_d=self.d_model//self.n_head# 每个head的维度\n",
    "        Q=self.w_q(x)\n",
    "        K=self.w_k(x)\n",
    "        V=self.w_v(x)\n",
    "        \n",
    "        Q=Q.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        K=K.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        V=V.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        \n",
    "        A=(K.T).dot(Q)/nn.sqrt(self.d_model)\n",
    "        A_hat=self.softmax(A)\n",
    "        B=A_hat.dot(V)  \n",
    "        return B      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个自定义的神经网络模块时，通常会继承 torch.nn.Module\n",
    "\n",
    "nn.Module 是 PyTorch 中所有神经\n",
    "\n",
    "可以通过torch.save()和torch.load()保存恢复模型\n",
    "继承 nn.Module 后，你可以通过以下方式扩展它\n",
    "* 定义自定义的网络结构（子模块）。\n",
    "* 定义自定义的前向传播逻辑（forward 方法）。\n",
    "\n",
    "这是 multi_head_attention 类的初始化方法，用于定义该模块的属性和子模块\n",
    "\n",
    "`def __init__(self,) -> None:`\n",
    "\n",
    "除了 self 参数外，你可以添加其他自定义的参数。这些参数可以用来配置类的行为、初始化特定的属性或子模块。\n",
    "\n",
    "super().__init__() 是一个关键步骤，用于调用父类（nn.Module）的初始化方法：\n",
    "\n",
    "`super(multi_head_attention, self).__init__()`\n",
    "\n",
    "如果没有调用 super().__init__()，nn.Module 的一些功能（如参数管理、设备分配）将无法正常工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT的multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化多头注意力模块。\n",
    "        \n",
    "        Args:\n",
    "        - embed_dim (int): 输入的嵌入维度（d_model）。\n",
    "        - num_heads (int): 注意力头的数量。\n",
    "        - dropout (float): Dropout 的比例。\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # 验证 embed_dim 是否可以被 num_heads 整除\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim 必须能够整除 num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads  # 每个头的维度 d_k\n",
    "        \n",
    "        # 定义 Q, K, V 的权重\n",
    "        self.W_Q = nn.Linear(embed_dim, embed_dim)  # (embed_dim, embed_dim)\n",
    "        self.W_K = nn.Linear(embed_dim, embed_dim)  # (embed_dim, embed_dim)\n",
    "        self.W_V = nn.Linear(embed_dim, embed_dim)  # (embed_dim, embed_dim)\n",
    "        \n",
    "        # 输出权重\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)  # (embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        Args:\n",
    "        - x (Tensor): 输入张量，形状为 (batch_size, seq_length, embed_dim)。\n",
    "        \n",
    "        Returns:\n",
    "        - out (Tensor): 输出张量，形状为 (batch_size, seq_length, embed_dim)。\n",
    "        - attention (Tensor): 注意力矩阵，形状为 (batch_size, num_heads, seq_length, seq_length)。\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        \n",
    "        # 计算 Q, K, V\n",
    "        Q = self.W_Q(x)  # (batch_size, seq_length, embed_dim)\n",
    "        K = self.W_K(x)  # (batch_size, seq_length, embed_dim)\n",
    "        V = self.W_V(x)  # (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # 拆分为多头\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        \n",
    "        # 点积注意力计算\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention = torch.softmax(attention_scores, dim=-1)  # 归一化\n",
    "        attention = self.dropout(attention)  # Dropout\n",
    "        \n",
    "        # 加权 V\n",
    "        out = torch.matmul(attention, V)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        \n",
    "        # 合并多头\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # 输出线性变换\n",
    "        out = self.fc_out(out)  # (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        return out, attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
