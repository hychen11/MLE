{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group query attention其实就是序列长度分为n组，然后每组内对键和值进行聚合，mean，比如W_k *X后得到的K取mean \n",
    "\n",
    "\n",
    "MHA 就是一个a有n head个输出qi i～0-（nhead-1），然后计算出n head个结果，直接concat起来乘W_v得到结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head,n_group)->None:\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_head=n_head\n",
    "        self.n_group=n_group\n",
    "        \n",
    "        assert d_model%n_head==0\n",
    "        #一个group里包含几个head\n",
    "        self.n_head_groups=self.n_head//self.n_group\n",
    "        #一个head的维度\n",
    "        self.head_dim=d_model//n_head\n",
    "        \n",
    "        self.w_q=nn.Linear(d_model,d_model)\n",
    "        self.w_k=nn.Linear(d_model,self.n_head_groups*self.head_dim)\n",
    "        self.w_v=nn.Linear(d_model,self.n_head_groups*self.head_dim)\n",
    "        \n",
    "        self.fc=nn.Linear(d_model,d_model)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "        \n",
    "    def expand(self,data):\n",
    "        batch,time=data.shape[0],data.shape[2]\n",
    "        data=data[:,:,None,:,:].expand(batch,self.n_group,self.n_head_groups,time,self.head_dim).contiguous()\n",
    "        data=data.view(batch,self.n_group*self.n_head_groups,time,self.head_dim)\n",
    "        return data\n",
    "    \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        q=self.w_q(q)\n",
    "        k=self.w_k(k)\n",
    "        v=self.w_v(v)\n",
    "        \n",
    "        batch=q.shape[0]\n",
    "        q=q.view(batch,-1,self.n_group*self.n_head_groups,self.head_dim).permute(0,2,1,3)\n",
    "        #这里取mean了，所以只有n_group\n",
    "        k=k.view(batch,-1,self.n_group,self.head_dim).permute(0,2,1,3)\n",
    "        v=v.view(batch,-1,self.n_group,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        k=self.expand(k)\n",
    "        v=self.expand(v)\n",
    "        \n",
    "        score=q@k.transpose(-1,-2)/(self.head_dim**0.5)\n",
    "        if mask is not None:\n",
    "            mask=mask.masked_fill(mask==0,-1e9)\n",
    "        score=self.softmax(score)@v\n",
    "        score.permute(0,2,1,3).contiguous().view(batch,-1,self.d_model)\n",
    "        \n",
    "        output=self.fc(score)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_groups):\n",
    "        super(GroupQueryAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Learnable linear projections for queries, keys, and values\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Linear projection for the output\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Linear projections\n",
    "        queries = self.q_proj(x)  # (batch_size, seq_len, embed_dim)\n",
    "        keys = self.k_proj(x)     # (batch_size, seq_len, embed_dim)\n",
    "        values = self.v_proj(x)   # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Group keys and values (average within groups)\n",
    "        keys = keys.view(batch_size, self.num_heads, self.num_groups, -1, self.head_dim).mean(dim=3)\n",
    "        values = values.view(batch_size, self.num_heads, self.num_groups, -1, self.head_dim).mean(dim=3)\n",
    "\n",
    "        # Attention scores (scaled dot-product)\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch_size, num_heads, seq_len, num_groups)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)  # Normalize over groups\n",
    "\n",
    "        # Attention output\n",
    "        context = torch.matmul(attn_probs, values)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Combine heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.out_proj(context)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 8\n",
    "    seq_len = 128\n",
    "    embed_dim = 256\n",
    "    num_heads = 8\n",
    "    num_groups = 4\n",
    "\n",
    "    x = torch.rand(batch_size, seq_len, embed_dim)\n",
    "    gqa = GroupQueryAttention(embed_dim, num_heads, num_groups)\n",
    "\n",
    "    output = gqa(x)\n",
    "    print(\"Output shape:\", output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
